# -*- coding: utf-8 -*-
"""text_embedding_word2vec W2Lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pLPE_g7w703j67tZh_4RzXaVlvuofoPT

Stored the file in gdrive. Mounted the gdrive for access.
"""

import gzip
import gensim
import logging
from google.colab import drive
drive.mount('/content/drive/');

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

import warnings
warnings.filterwarnings('ignore')

"""Now, let's take a closer look at this data below by printing the first line. You can see that this is a pretty hefty review. The Data we used is of Review Data of Hotels and Cars. Stored in file reviews_data.txt.gz"""

cd "/content/drive/My Drive/Colab Notebooks/DL Course/"

data_file="reviews_data.txt.gz"

with gzip.open ('reviews_data.txt.gz', 'rb') as f:
    for i,line in enumerate (f):
        print(line)
        break

"""This method below reads the input file which is in gzip format"""

def read_input(input_file):
    
    logging.info("reading file {0}...this may take a while".format(input_file))
    
    with gzip.open (input_file, 'rb') as f:
        for i, line in enumerate (f): 

            if (i%10000==0):
                logging.info ("read {0} reviews".format (i))
            # do some pre-processing and return a list of words for each review text
            yield gensim.utils.simple_preprocess (line)

# read the tokenized reviews into a list
# each review item becomes a serries of words
# so this becomes a list of lists
documents = list (read_input (data_file))
logging.info ("Done reading data file")

print(documents[1])

model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)
model.train(documents,total_examples=len(documents),epochs=10)

"""Below example shows a simple case of looking up words similar to the word dirty. All we need to do here is to call the most_similar function and provide the word "dirty" as the positive example. This returns the top 10 similar words."""

w1 = "dirty"
model.wv.most_similar (positive=w1)

"""Heres few more example . Similarity for "polite", "france" and "shocked"."""

# look up top 15 words similar to 'polite'
w1 = ["polite"]
model.wv.most_similar (positive=w1,topn=15)

# look up top 30 words similar to 'france'
w1 = ["france"]
model.wv.most_similar (positive=w1,topn=30)

# look up top 26 words similar to 'shocked'
w1 = ["shocked"]
model.wv.most_similar (positive=w1,topn=26)

"""WE can even specify several positive examples to get things that are related in the provided context and provide negative examples to say what should not be considered as related. In the example below we are asking for all items that relate to bed only:"""

# get everything related to stuff on the bed
w1 = ["bed",'sheet','pillow']
w2 = ['couch']
model.wv.most_similar (positive=w1,negative=w2,topn=10)

"""We can even use the Word2Vec model to return the similarity between two words that are present in the vocabulary."""

# similarity between two different words
model.wv.similarity(w1="dirty",w2="smelly")

# similarity between two identical words
model.wv.similarity(w1="dirty",w2="dirty")

# similarity between two unrelated words
model.wv.similarity(w1="dirty",w2="clean")

"""Under the hood, the above three snippets computes the cosine similarity between the two specified words using word vectors of each. From the scores, it makes sense that dirty is highly similar to smelly but dirty is dissimilar to clean. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0].

We can even use Word2Vec to find odd items given a list of items.
"""

# Which one is the odd one out in this list?
model.wv.doesnt_match(["cat","dog","france"])

# Which one is the odd one out in this list?
model.wv.doesnt_match(["bed","pillow","duvet","shower"])

"""To train the model earlier, we had to set some parameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model.




```
model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)
```

`size`
The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me.

`window`
The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window.

`min_count`
Minimium frequency count of words. The model would ignore words that do not statisfy the min_count. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.

`workers`
How many threads to use behind the scenes?
"""